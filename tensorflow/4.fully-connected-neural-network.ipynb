{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import numpy as np\n",
    "# mnist data\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "(60000, 28, 28) (60000, 10)\n",
      "(60000, 28, 28, 1) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=True, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match generel framework architecture\n",
    "train_images, test_images = train_images.reshape(60000, 28, 28, 1), test_images.reshape(10000, 28, 28, 1)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "For a better understanding of neural networks you will start to implement your own framework. The given notebook explaines some core functions and concetps of the framework, so you all have the same starting point. The  Pipeline will be: \n",
    "\n",
    "**define a model architecture -> construct a neural network from the model -> define a evaluation citeria -> optimize the network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom architecture\n",
    "To create a custom model you have to define layers and activation functions that can be used to do so. Layers and activation functions are modeled as objects. Each object that you want to use has to implement a `forward` that is used by the `NeuralNetwork` class. Additionally the `self.params` attribute is mandatory to meet the specification of the `NeuralNetwork` class. It is used to store all learnable parameters that you need for the optimization algorithm. We implement our neural network so that we can use the objects as building blocks and stack them up to create a custom model. \n",
    "\n",
    "#### Layers  Class\n",
    "The file `layer.py` contains implementations of neural network layers and regularization techniques that can be inserted as layers into the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    ''' Flatten layer used to reshape inputs into vector representation\n",
    "    \n",
    "    Layer should be used in the forward pass before a dense layer to \n",
    "    transform a given tensor into a vector. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Reshapes a n-dim representation into a vector \n",
    "            by preserving the number of input rows.\n",
    "        \n",
    "        Args:\n",
    "            X: Images set\n",
    "    \n",
    "        Returns:\n",
    "            X_: Matrix with images in a flatten represenation\n",
    "            \n",
    "        Examples:\n",
    "            [10000, 1, 28, 28] -> [10000,784]\n",
    "        '''\n",
    "        return tf.reshape(X, [-1, X.shape[1] * X.shape[2] * X.shape[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we are going to define some activation functions which we obviously going to use on top of the layers. Thus we add to the constructor a parameter ```activation_func``` to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected():\n",
    "    ''' Fully connected layer implemtenting linear function hypothesis \n",
    "        in the forward pass and its derivation in the backward pass.\n",
    "    '''\n",
    "    def __init__(self, in_size, out_size, activation_func=None,stddev=0.1):\n",
    "        ''' Initilize all learning parameters in the layer\n",
    "        \n",
    "        Weights will be initilized with modified Xavier initialization.\n",
    "        Biases will be initilized with zero. \n",
    "        '''\n",
    "        self.W = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=stddev))\n",
    "        self.b = tf.Variable(tf.ones([out_size])/10)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.activation_func = activation_func\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Linear combiationn of images, weights and bias terms\n",
    "            \n",
    "        Args:\n",
    "            X: Matrix of images (flatten represenation)\n",
    "    \n",
    "        Returns:\n",
    "            out: Sum of X*W+b  \n",
    "        '''\n",
    "        Z = tf.matmul(X, self.W) + self.b\n",
    "        if self.activation_func is None:\n",
    "            return Z\n",
    "        else:\n",
    "            return self.activation_func.forward(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "The file `activation_func.py` contains implementations of activation functions you can use as a none linearity in your network. The functions work on the basis of matrix operations and not discret values, so that these can also be inserted as a layer. As an example the ReLU and Softmax function is implemented:\n",
    "\n",
    "$$\n",
    "f ( x ) = \\left\\{ \\begin{array} { l l } { x } & { \\text { if } x > 0 } \\\\ { 0 } & { \\text { otherwise } } \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    ''' Implements activation function rectified linear unit (ReLU) \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    ''' Implements activation function softmax\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        return tf.nn.softmax(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "Implementations of loss functions can be found in `loss_func.py`. A loss function object defines the criteria your network is evaluated during the optimization process and also contains score functions that can be used as classification criteria for predictions with the final model. Therefore it is necessary to create a loss function object and provide to the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_functions:\n",
    "    ''' Implements different typs of loss functions for neural networks\n",
    "    '''\n",
    "\n",
    "    def cross_entropy(X, y):\n",
    "        ''' Computes loss and prepares dout for backprop \n",
    "\n",
    "        https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        '''\n",
    "        cross_entropy = -tf.reduce_mean(y * tf.log(X)) * 10\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    ''' Creates a neural network from a given layer architecture \n",
    "\n",
    "    This class is suited for fully connected network and\n",
    "    convolutional neural network architectures. It connects \n",
    "    the layers and passes the data from one end to another.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        ''' Setup a global parameter list and initilize a\n",
    "            score function that is used for predictions.\n",
    "\n",
    "        Args:\n",
    "            layer: neural network architecture based on layer and activation function objects\n",
    "            score_func: function that is used as classifier on the output\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            if len(layer.params) > 0:\n",
    "                self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Pass input X through all layers in the network \n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Run a forward pass and use the score function to classify \n",
    "            the output.\n",
    "        '''\n",
    "        temp = tf.placeholder(tf.float32, X.shape)\n",
    "        pred = self.forward(temp)\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            pred = sess.run(pred, feed_dict={temp: X})\n",
    "        return np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with SGD\n",
    "The file `optimizer.py` contains implementations of optimization algorithms. Your optimizer needs your custom `network`, `data` and `loss function` and some additional hyperparameter as arguments to optimzie your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "\n",
    "    def get_minibatches(X, y, batch_size):\n",
    "        ''' Decomposes data set into small subsets (batch)\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i + batch_size, :, :, :]\n",
    "            y_batch = y[i:i + batch_size, ]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches\n",
    "\n",
    "    def calculate_gradient(network, loss_function):\n",
    "        grad = []\n",
    "        for param in network.params:\n",
    "            W, b = param\n",
    "            dW = tf.gradients(loss_function, W)\n",
    "            db = tf.gradients(loss_function, b)\n",
    "            grad.append([dW, db])\n",
    "        return grad\n",
    "\n",
    "    def sgd(network, X_train, y_train, loss_function, batch_size=32, epoch=100, learning_rate=0.01, X_test=None, y_test=None, verbose=None):\n",
    "        ''' Optimize a given network with stochastic gradient descent \n",
    "        '''\n",
    "        X_shape, y_shape = [None], [None]\n",
    "        for x in X_train.shape[1:]:\n",
    "            X_shape.append(x)\n",
    "        for y in y_train.shape[1:]:\n",
    "            y_shape.append(y)\n",
    "        X = tf.placeholder(tf.float32, X_shape)\n",
    "        Y = tf.placeholder(tf.float32, y_shape)\n",
    "\n",
    "        loss = loss_function(network.forward(X), Y)\n",
    "        grads = Optimizer.calculate_gradient(network, loss)\n",
    "\n",
    "        minibatches = Optimizer.get_minibatches(X_train, y_train, batch_size)\n",
    "        for i in range(epoch):\n",
    "            if verbose:\n",
    "                print('Epoch', i + 1)\n",
    "            for X_mini, y_mini in tqdm(minibatches):\n",
    "                for param, grad in zip(network.params, grads):\n",
    "                    with tf.Session() as sess:\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        sess.run(init)\n",
    "                        sess_grad = sess.run(grad, feed_dict={X: X_mini, Y: y_mini})\n",
    "                    for i in range(len(sess_grad)):\n",
    "                        param[i] = param[i] - learning_rate * sess_grad[i][0]\n",
    "            if verbose:\n",
    "                with tf.Session() as sess:\n",
    "                    init = tf.global_variables_initializer()\n",
    "                    sess.run(init)\n",
    "                    sess_loss = sess.run(loss, feed_dict={X: X_mini, Y: y_mini})\n",
    "                train_acc = np.mean(np.argmax(y_train, axis=1) == network.predict(X_train))\n",
    "                test_acc = np.mean(np.argmax(y_test, axis=1) == network.predict(X_test))\n",
    "                print(\"Loss = {0} :: Training = {1} :: Test = {2}\".format(\n",
    "                    sess_loss, train_acc, test_acc))\n",
    "        return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together\n",
    "Now you have parts together to create and train a fully connected neural network. First, you have to define an individual network architecture by flatten the input and stacking fully connected layer with activation functions. Your custom architecture is given to a `NeuralNetwork` object that handles the inter-layer communication during the forward and backward pass. Finally, you optimize the model with a chosen algorithm, here stochastic gradient descent. That kind of pipeline is similar to the one you would create with a framework like Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 300, activation_func=ReLU())\n",
    "    hidden_02 = FullyConnected(300, 200, activation_func=ReLU())\n",
    "    hidden_03 = FullyConnected(200, 100, activation_func=ReLU())\n",
    "    ouput = FullyConnected(100, 10, activation_func=Softmax())\n",
    "    return [flat, hidden_01, hidden_02, hidden_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [02:29<00:00,  1.57it/s]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.413881778717041 :: Training = 0.12285 :: Test = 0.1105\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [06:08<00:00,  1.57s/it]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.3438308238983154 :: Training = 0.10123333333333333 :: Test = 0.1133\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [09:47<00:00,  2.50s/it]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.551849126815796 :: Training = 0.12831666666666666 :: Test = 0.0889\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [14:00<00:00,  3.58s/it]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.560546398162842 :: Training = 0.08156666666666666 :: Test = 0.1382\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [18:34<00:00,  4.74s/it]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.3954720497131348 :: Training = 0.06816666666666667 :: Test = 0.1136\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [22:54<00:00,  5.85s/it]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.367936849594116 :: Training = 0.1372 :: Test = 0.0882\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 194/235 [21:40<04:34,  6.71s/it]"
     ]
    }
   ],
   "source": [
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images, train_labels, Loss_functions.cross_entropy, batch_size=256, epoch=10, learning_rate=0.01, X_test=test_images, y_test=test_labels, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
