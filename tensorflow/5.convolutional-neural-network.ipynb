{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "train shapes: (60000, 28, 28, 1) (60000, 10)\n",
      "test shapes: (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = Mnist().get_all_data(one_hot_enc=True, flatten=False)\n",
    "train_images, test_images = train_images.reshape(60000, 28, 28, 1), test_images.reshape(10000,28,28,1)\n",
    "print('train shapes:', train_images.shape, train_labels.shape)\n",
    "print('test shapes:', test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network architecture:\n",
    "#\n",
    "# · · · · · · · · · ·      (input data, 1-deep)                 X [batch, 28, 28, 1]\n",
    "#   @ @ @ @ @ @ @ @     -- conv. layer stride 2                 W1 [3, 3, 1, 64]       B1 [64]\n",
    "#   ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                             Y1 [batch, 14, 14, 64]\n",
    "#     @ @ @ @ @ @       -- conv. layer stride 2                 W2 [3, 3, 64, 32]      B2 [32]\n",
    "#     ∶∶∶∶∶∶∶∶∶∶∶                                               Y2 [batch, 7, 7, 32] => reshaped to YY [batch, 7*7*32]\n",
    "#      \\x/x\\x\\x/        -- fully connected layer (relu)         W3 [7*7*32, 256]       B3 [256]\n",
    "#       · · · ·                                                 Y3 [batch, 256]\n",
    "#       \\x/x\\x/         -- fully connected layer (softmax)      W4 [256, 10]           B4 [10]\n",
    "#        · · ·                                                  Y4 [batch, 10]\n",
    "\n",
    "# convolution layer weights\n",
    "W1 = tf.Variable(tf.truncated_normal([3, 3, 1, 64], stddev=0.1)) \n",
    "B1 = tf.Variable(tf.ones([64])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([3, 3, 64, 32], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([32])/10)\n",
    "\n",
    "# fully connected weights\n",
    "W3 = tf.Variable(tf.truncated_normal([7 * 7 * 32, 256], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([256])/10)\n",
    "W4 = tf.Variable(tf.truncated_normal([256, 10], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# The model\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, 2, 2, 1], padding='SAME') + B1)\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(conv1, W2, strides=[1, 2, 2, 1], padding='SAME') + B2)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "flatten = tf.reshape(conv2, shape=[-1, 7 * 7 * 32])\n",
    "\n",
    "# fully connected\n",
    "dense1 = tf.nn.relu(tf.matmul(flatten, W3) + B3)\n",
    "output = tf.matmul(dense1, W4) + B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our **Use Case**, we need a kind of prediction layer on top of our output layer. We use a, so called, Softmax layer or the prediction which we put on top of the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Optimizer\n",
    "\n",
    "In general, the loss functions tells us how \"good\" or how \"bad\" our neural network is. This function is then minimized by the neural network so that the neural network gives us the best performance based on the defined loss function. For this purpose we are going to use the cross entropy loss function which is used very heavily in neural networks and seems to work very well.\n",
    "\n",
    "**Note:** TensorFlow provides the ```softmax_cross_entropy_with_logits``` function to avoid numerical stability problems with log(0) which is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=Y)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the gradient descent method **Adam** to minimize our loss function. We also add a learning rate with an exponential decay. In our setting we start at a learning rate of $0.003$ and exponentially reduce it to $0.00001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step for variable learning rate\n",
    "step = tf.placeholder(tf.int32)\n",
    "\n",
    "# the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/2000)\n",
    "learning_rate = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/np.exp(1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a ```accuracy``` so that we can see whether our network actually improves while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:40<00:00,  5.84it/s]\n",
      "  0%|          | 1/235 [00:00<00:36,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 813.623\n",
      "test accuracy 0.0729 train accuracy 0.072783336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:41<00:00,  5.62it/s]\n",
      "  0%|          | 1/235 [00:00<00:35,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1208.2542\n",
      "test accuracy 0.1412 train accuracy 0.13365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:41<00:00,  5.71it/s]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 752.2262\n",
      "test accuracy 0.1732 train accuracy 0.16976666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:57<00:00,  4.12it/s]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 1309.861\n",
      "test accuracy 0.047 train accuracy 0.051916666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:04<00:00,  3.63it/s]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 865.4551\n",
      "test accuracy 0.1227 train accuracy 0.11986667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [01:10<00:00,  3.34it/s]\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 1136.0391\n",
      "test accuracy 0.0991 train accuracy 0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 179/235 [00:58<00:18,  3.04it/s]"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for e in range(epochs):\n",
    "    for batch_i in tqdm(range(0, train_images.shape[0], batch_size)):\n",
    "        data, label = train_images[batch_i:batch_i + batch_size], train_labels[batch_i:batch_i + batch_size]\n",
    "\n",
    "        # run the computational graph and calculate loss + training step\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            # optimizer will not return something which is why we store it into a variable called empty\n",
    "            loss, empty = sess.run([cross_entropy, train_step], feed_dict={X: data, Y: label, step: e})\n",
    "        # append to loss history\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        train_acc = sess.run(accuracy, feed_dict={X:train_images, Y: train_labels})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X:test_images, Y: test_labels})\n",
    "    print('epoch:', e, 'loss:', loss)\n",
    "    print('test accuracy', test_acc, 'train accuracy', train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "Let us look at the optimization results. Final loss tells us how far we could reduce costs during traning process. Further we can use the first loss value as a sanity check and validate our implementation of the loss function works as intended. Recall loss value after first iteration should be $ log\\:c$ with $c$ being number of classes. To visulize the whole tranings process we can plot losss values from each iteration as a loss curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check loss after last and first iteration\n",
    "print('last iteration loss:',loss_history[-1])\n",
    "print('first iteration loss:',loss_history[0])\n",
    "# Plot a loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation above gave us some inside about the optimization process but did not quantified our final model. One possibility is to calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    acc = sess.run(accuracy, feed_dict={X:test_images, Y: test_labels,  pkeep: 1})\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
