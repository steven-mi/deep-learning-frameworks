{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected neural network with Tensorflow for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Tensorflow is is a symbolic math library and one of the widely used libraries for implementing Machine learning/other algorithms involving large number of mathematical operations. Tensorflow was developed by Google and it’s open source now. It is used for both research and production at Google e.g. for implementing Machine learning in almost all applications \n",
    "- Google photos \n",
    "- Google voice search \n",
    "\n",
    "In this notebook we are going to build a fully connected neural network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "The MNIST dataset is a classic Machine Learning dataset you can get it and more information about it from the website of Yann Lecun. MNIST contains handwrittin digits and is split into a tranings set of 60000 examples and a test set of 10000 examples. We use the ```deep_teaching_commons``` package to load the MNIST dataset in a convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "train shapes: (60000, 28, 28, 1) (60000, 10)\n",
      "test shapes: (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = Mnist().get_all_data(one_hot_enc=True, flatten=False)\n",
    "train_images, test_images = train_images.reshape(60000, 28, 28, 1), test_images.reshape(10000,28,28,1)\n",
    "print('train shapes:', train_images.shape, train_labels.shape)\n",
    "print('test shapes:', test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "So far we have used numpy arrays to manage our data, but in order to build a model in tensorflow we need another structure, the placeholder. A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected neural network for MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weights\n",
    "By initializing the weights of our neural network (the learnable parameter), we already define how our network is going to look like. We decided to use a neural network with 3 layer with a ReLU and Dropout function on top of each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([784, 512], stddev=0.1)) \n",
    "B1 = tf.Variable(tf.zeros([512]))\n",
    "W2 = tf.Variable(tf.truncated_normal([512, 256], stddev=0.1))  \n",
    "B2 = tf.Variable(tf.zeros([256]))\n",
    "W3 = tf.Variable(tf.truncated_normal([256, 128], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([128]))\n",
    "W4 = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1))\n",
    "B4 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique which tries to prevent overfitting. Overfitting means that our network can't perform very well on images it haven't seen before which is obviously really bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "We have, as described above, a 3 layer fully connected neural network with ReLU and Dropout on top of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = tf.reshape(X, [-1, 784])\n",
    "hidden1 = tf.nn.relu(tf.matmul(flatten, W1) + B1)\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + B2)\n",
    "dropout2 = tf.nn.dropout(hidden2, pkeep)\n",
    "hidden3 = tf.nn.relu(tf.matmul(dropout2, W3) + B3)\n",
    "dropout3 = tf.nn.dropout(hidden3, pkeep)\n",
    "output = tf.nn.relu(tf.matmul(dropout3, W4) + B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our **Use Case**, we need a kind of prediction layer on top of our output layer. We use a, so called, Softmax layer or the prediction which we put on top of the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss function\n",
    "In general, the loss functions tells us how \"good\" or how \"bad\" our neural network is. This function is then minimized by the neural network so that the neural network gives us the best performance based on the defined loss function. For this purpose we are going to use the cross entropy loss function which is used very heavily in neural networks and seems to work very well.\n",
    "\n",
    "**Note:** TensorFlow provides the ```softmax_cross_entropy_with_logits``` function to avoid numerical stability problems with log(0) which is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=Y)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "We are going to use the gradient descent method **Adam** to minimize our loss function. We also add a learning rate with an exponential decay. In our setting we start at a learning rate of $0.003$ and exponentially reduce it to $0.00001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step for variable learning rate\n",
    "step = tf.placeholder(tf.int32)\n",
    "\n",
    "# the learning rate is: # 0.0001 + 0.003 * (1/e)^(step/2000)\n",
    "learning_rate = 0.001 +  tf.train.exponential_decay(0.003, step, 2000, 1/np.exp(1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a ```accuracy``` so that we can see whether our network actually improves while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 144.73it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 151.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 155.15it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 154.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 155.43it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 155.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 152.81it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 154.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 154.67it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 154.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 154.47it/s]\n",
      "  7%|▋         | 17/235 [00:00<00:01, 160.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 156.92it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 155.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 154.24it/s]\n",
      "  6%|▋         | 15/235 [00:00<00:01, 148.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 153.06it/s]\n",
      "  7%|▋         | 16/235 [00:00<00:01, 155.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:01<00:00, 152.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 2.3025854\n",
      "test accuracy 0.098 train accuracy 0.09871667\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "for e in range(epochs):\n",
    "    for batch_i in tqdm(range(0, train_images.shape[0], batch_size)):\n",
    "        data, label = train_images[batch_i:batch_i + batch_size], train_labels[batch_i:batch_i + batch_size]\n",
    "        # run the computational graph and calculate loss + training step\n",
    "        # optimizer will not return something which is why we store it into a variable called empty\n",
    "        loss, empty = sess.run([cross_entropy, train_step], feed_dict={X: data, Y: label,  pkeep: 1, step: e})\n",
    "        # append to loss history\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    train_acc = sess.run(accuracy, feed_dict={X:train_images, Y: train_labels,  pkeep: 1})\n",
    "    test_acc = sess.run(accuracy, feed_dict={X:test_images, Y: test_labels,  pkeep: 1})\n",
    "    print('epoch:', e, 'loss:', loss)\n",
    "    print('test accuracy', test_acc, 'train accuracy', train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "Let us look at the optimization results. Final loss tells us how far we could reduce costs during traning process. Further we can use the first loss value as a sanity check and validate our implementation of the loss function works as intended. To visulize the whole tranings process we can plot losss values from each iteration as a loss curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last iteration loss: 2.3025854\n",
      "first iteration loss: 205.22441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'iterations')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check loss after last and first iteration\n",
    "print('last iteration loss:',loss_history[-1])\n",
    "print('first iteration loss:',loss_history[0])\n",
    "# Plot a loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation above gave us some inside about the optimization process but did not quantified our final model. One possibility is to calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.098\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(accuracy, feed_dict={X:test_images, Y: test_labels,  pkeep: 1})\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
